The k-Nearest Neighbor (k-NN) Classifier Algorithm

Given a training set X_train with labels y_train, and given a new instance X_test to be classified:

1. Find the most similar instances (let's call them X_NN) to X_test that are in X_train

2. Get the labels y_NN for the instances in X_NN

3. Predict the label for X_test by combining the lables y_NN e.g. simple majority vote

As the k approches to 1 (decreases), the model is more likely to overfit as it would tend to capture local trend more effectively - the prediction boundary is more curved. 


Important parameters in KNeighborClassifier and KNeighborsRegressor:
Model Complexity:
- n_neighbors: number of nearest neighbors (K) to consider: default = 5

Model fitting:
- metric: distance function between data points: defauls: Minkowski distance with power parameter p = 2 (Euclidean)
